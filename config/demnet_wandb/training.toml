batch_size = 69
lr = 0.01
epochs = 800000000000
optimizer_weight_decay = 0.01
device = "mps"
epoch_to_save_model = 10
path_to_save_model = "model_weights"
measure_metrics_during_training = true
print_var = true
seed = -1
wandb_training = false
project_name = "demnet_training_ADNI"
model_artifact_name = "demnet_training_ADNI"
log_freq = 1
name_training_run = ""
notes = ""
debug = false
use_scheduler = true

# lr scheduler config
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# CosineAnnealingLR Config
# [train_config.lr_scheduler_config]
# name = 'CosineAnnealingLR'
# T_max = 20
# eta_min = 1e-5
# gamma = 0.96 
# step_size = 5
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# CosineAnnealingLR + ExponentialLR
[train_config.lr_scheduler_config]
name = 'ChainedScheduler'
[train_config.lr_scheduler_config.list_config_schedulers.config_1]
name = 'CosineAnnealingLR'
T_max = 9
eta_min = 1e-5
[train_config.lr_scheduler_config.list_config_schedulers.config_2]
name = 'ExponentialLR'
gamma = 0.93
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# CosineAnnealingWarmRestarts 
# [train_config.lr_scheduler_config]
# name = 'CosineAnnealingWarmRestarts'
# T_0 = 10
# T_mult = 2
# eta_min = 1e-5
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
# [train_config.lr_scheduler_config]
# name = ' CyclicLR'
# base_lr = 1e-5
# max_lr = starting_lr
# gamma = gamma
# mode = 'exp_range'
# step_size_up = 10
# step_size_down = 15
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
